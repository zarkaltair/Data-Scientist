# -*- coding: utf-8 -*-
"""02_learn_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UNC5j_7UafrpTuAPukbUNWV7rWuzLOeM
"""

import torch

x = torch.tensor(
    [[1., 2., 3., 4.],
    [5., 6., 7., 8.],
    [9., 10., 11., 12.]], requires_grad=True)

device = torch.device('cuda:0'
                      if torch.cuda.is_available()
                      else 'cpu')
x = x.to(device)

function = 10 * (x**2).sum()

function.backward()

print(x.grad, '<-gradient')

x.data -= 0.001 * x.grad

x.grad.zero_()

x = torch.tensor([8., 8.], requires_grad=True)

optimizer = torch.optim.SGD([x], lr=0.001)

def function_parabola(variable):
    return 10 * (variable**2).sum()

def make_gradient_step(function, variable):
    function_result = function(variable)
    function_result.backward()
    optimizer.step()
    optimizer.zero_grad()

var_history = []
fn_history = []

for i in range(500):
    var_history.append(x.data.numpy().copy())
    fn_history.append(function_parabola(x).data.cpu().numpy().copy())
    make_gradient_step(function_parabola, x)

import numpy as np
import matplotlib.pyplot as plt

def show_contours(objective,
                  x_lims=[-10.0, 10.0], 
                  y_lims=[-10.0, 10.0],
                  x_ticks=100,
                  y_ticks=100):
    x_step = (x_lims[1] - x_lims[0]) / x_ticks
    y_step = (y_lims[1] - y_lims[0]) / y_ticks
    X, Y = np.mgrid[x_lims[0]:x_lims[1]:x_step, y_lims[0]:y_lims[1]:y_step]
    res = []
    for x_index in range(X.shape[0]):
        res.append([])
        for y_index in range(X.shape[1]):
            x_val = X[x_index, y_index]
            y_val = Y[x_index, y_index]
            res[-1].append(objective(np.array([[x_val, y_val]]).T))
    res = np.array(res)
    plt.figure(figsize=(7,7))
    plt.contour(X, Y, res, 100)
    plt.xlabel('$x_1$')
    plt.ylabel('$x_2$')

show_contours(function_parabola)
plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');

plt.figure(figsize=(7,7))
plt.plot(fn_history)
plt.xlabel('step')
plt.ylabel('function value');

plt.figure(figsize=(7,7))
plt.semilogy(fn_history)
plt.xlabel('step')
plt.ylabel('function value');

def function_skewed(variable):
    gramma = torch.tensor([[1., -1.], [1., 1.]]) @ torch.tensor([[1.0, 0.0], [0.0, 4.0]])
    res = 10 * (variable.unsqueeze(0) @ (gramma @ variable.unsqueeze(1))).sum()
    return res

def function_skewed_np(variable):
    gramma = np.array([[1, -1], [1, 1]]) @ np.array([[1.0, 0.0], [0.0, 4.0]])
    res = 10 * (variable.transpose(1, 0) @ (gramma @ variable)).sum()
    return res

show_contours(function_skewed_np)

x = torch.tensor(
    [8., 8.], requires_grad=True)
var_history = []
fn_history = []

optimizer = torch.optim.SGD([x], lr=0.001)

for i in range(500):
    var_history.append(x.data.cpu().numpy().copy())
    fn_history.append(function_skewed(x).data.cpu().numpy().copy())
    make_gradient_step(function_skewed, x)

show_contours(function_skewed_np)
plt.scatter(np.array(var_history)[:,0], np.array(var_history)[:,1], s=10, c='r');



"""Task"""

w = torch.tensor([
      [5., 10.],
      [1., 2.]], requires_grad=True)
opt = torch.optim.SGD([w], lr=0.001)

function = torch.log(torch.log(w+7)).prod()

function.backward()

print(w.grad)

for _ in range(500):
    function = (w+7).log().log().prod()
    function.backward()
    opt.step()
    opt.zero_grad()

print(w)



