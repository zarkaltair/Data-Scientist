# -*- coding: utf-8 -*-
"""05_learn_pytorch_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CfobyFzEYYqFmo65Yht_TQQ9FbXs7-Se
"""

import torch
import random
import numpy as np

random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True

import torchvision.datasets
MNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)
MNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)

X_train = MNIST_train.data
y_train = MNIST_train.targets
X_test = MNIST_test.data
y_test = MNIST_test.targets

X_train.dtype, y_train.dtype

X_train = X_train.float()
X_test = X_test.float()

X_train.shape, X_test.shape

y_train.shape, y_test.shape

# Commented out IPython magic to ensure Python compatibility.
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
matplotlib.rcParams['figure.figsize'] = (16.0, 8.0)

plt.imshow(X_train[0, :, :])
plt.show()
print(y_train[0])

X_train = X_train.reshape([-1, 28 * 28])
X_test = X_test.reshape([-1, 28 * 28])

class MNISTNet(torch.nn.Module):
    def __init__(self, n_hidden_neurons):
        super().__init__()
        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)
        self.activ1 = torch.nn.Sigmoid()
        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.activ1(x)
        x = self.fc2(x)
        return x
    
mnist_net = MNISTNet(100)

torch.cuda.is_available()

!nvidia-smi

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
mnist_net = mnist_net.to(device)
# list(mnist_net.parameters())

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)

batch_size = 1000

train_accuracy_history = []
train_loss_history = []

test_accuracy_history = []
test_loss_history = []

X_test = X_test.to(device)
y_test = y_test.to(device)

for epoch in range(200):
    order = np.random.permutation(len(X_train))

    arr_acc = []
    arr_loss = []
    for start_index in range(0, len(X_train), batch_size):
        optimizer.zero_grad()

        batch_indexes = order[start_index: start_index+batch_size]

        X_batch = X_train[batch_indexes].to(device)
        y_batch = y_train[batch_indexes].to(device)

        preds = mnist_net.forward(X_batch)

        loss_value = loss(preds, y_batch)
        arr_loss.append(loss_value)
        loss_value.backward()
        train_accuracy = (preds.argmax(dim=1)==y_batch).float().mean()
        arr_acc.append(train_accuracy)

        optimizer.step()

    train_accuracy_history.append(torch.mean(torch.Tensor(arr_acc)))
    train_loss_history.append(torch.mean(torch.Tensor(arr_loss)))
    
    test_preds = mnist_net.forward(X_test)
    test_loss_history.append(loss(test_preds, y_test))

    accuracy = (test_preds.argmax(dim=1)==y_test).float().mean()
    test_accuracy_history.append(accuracy)
    # print(accuracy)

plt.plot(test_accuracy_history, label='Test acc')
plt.plot(train_accuracy_history, label='Train acc')
plt.legend(loc='lower right')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show();

plt.plot(test_loss_history, label='Test loss')
plt.plot(train_loss_history, label='Train loss')
plt.legend(loc='upper right')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show();



# Commented out IPython magic to ensure Python compatibility.
# %%time
# from torch.optim import Adadelta, Adam, RMSprop, SGD
# 
# arr_optim = [Adadelta, Adam, RMSprop, SGD]
# 
# test_accuracy_history_adadelta = []
# test_accuracy_history_adam = []
# test_accuracy_history_rmsprop = []
# test_accuracy_history_sgd = []
# 
# for optim in arr_optim[::-1]:
#     print(optim)
#     mnist_net = MNISTNet(100)
#     
#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
#     mnist_net = mnist_net.to(device)
#     
#     loss = torch.nn.CrossEntropyLoss()
#     optimizer = optim(mnist_net.parameters(), lr=1.0e-3)
#     
#     batch_size = 1000
# 
#     X_test = X_test.to(device)
#     y_test = y_test.to(device)
# 
#     for epoch in range(150):
#         order = np.random.permutation(len(X_train))
# 
#         arr_acc = []
#         arr_loss = []
#         for start_index in range(0, len(X_train), batch_size):
#             optimizer.zero_grad()
# 
#             batch_indexes = order[start_index: start_index+batch_size]
# 
#             X_batch = X_train[batch_indexes].to(device)
#             y_batch = y_train[batch_indexes].to(device)
# 
#             preds = mnist_net.forward(X_batch)
# 
#             loss_value = loss(preds, y_batch)
#             arr_loss.append(loss_value)
#             loss_value.backward()
#             train_accuracy = (preds.argmax(dim=1)==y_batch).float().mean()
#             arr_acc.append(train_accuracy)
# 
#             optimizer.step()
# 
#         test_preds = mnist_net.forward(X_test)
#         accuracy = (test_preds.argmax(dim=1)==y_test).float().mean()
#         if optim == torch.optim.Adadelta:
#             test_accuracy_history_adadelta.append(accuracy)
#         elif optim == torch.optim.Adam:
#             test_accuracy_history_adam.append(accuracy)
#         elif optim == torch.optim.RMSprop:
#             test_accuracy_history_rmsprop.append(accuracy)
#         else:
#             test_accuracy_history_sgd.append(accuracy)

plt.plot(test_accuracy_history_adadelta, label='Test acc Adadelta')
plt.plot(test_accuracy_history_adam, label='Train acc Adam')
plt.plot(test_accuracy_history_rmsprop, label='Train acc RMSprop')
plt.plot(test_accuracy_history_sgd, label='Train acc SGD')
plt.legend(loc='lower right')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show();



x = torch.zeros([6000, 28, 28], dtype=torch.int32)

x.reshape(-1, 1, 1).shape

x.reshape(len(x[1]), len(x), len(x[2])).shape

x.reshape(-1, 6000).shape

x.shape

x.reshape(-1, 14, 32, 7).shape

x.reshape(-1, 9).shape

x.reshape(-1).shape